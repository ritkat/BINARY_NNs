{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NETy3NipmI1I"
      },
      "outputs": [],
      "source": [
        "# Time to do a neural network of MNIST\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.ao.quantization as quant\n",
        "from torch.autograd.function  import Function, InplaceFunction\n",
        "import torch.nn.init as init\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "acc_list = []\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.1\n",
        "epochs = 1\n",
        "\n",
        "# Transform for MNIST dataset (normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "class Binarize(InplaceFunction):\n",
        "\n",
        "    def forward(ctx,input,quant_mode='det',allow_scale=False,inplace=False):\n",
        "        ctx.inplace = inplace\n",
        "        if ctx.inplace:\n",
        "            ctx.mark_dirty(input)\n",
        "            output = input\n",
        "        else:\n",
        "            output = input.clone()\n",
        "\n",
        "        scale= output.abs().max() if allow_scale else 1\n",
        "\n",
        "        if quant_mode=='det':\n",
        "            return output.div(scale).sign().mul(scale)\n",
        "        else:\n",
        "            return output.div(scale).add_(1).div_(2).add_(torch.rand(output.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1).mul(scale)\n",
        "\n",
        "    def backward(ctx,grad_output):\n",
        "        #STE\n",
        "        grad_input=grad_output\n",
        "        return grad_input,None,None,None\n",
        "\n",
        "def binarized(input,quant_mode='det'):\n",
        "      return Binarize.apply(input,quant_mode)\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "        # self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Initialize weights with a uniform distribution centered around 0\n",
        "        init.uniform_(self.weight, a=-1, b=1)  # Uniform distribution between -1 and 1\n",
        "        if self.bias is not None:\n",
        "            init.constant_(self.bias, 0)  # Bias initialized to 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        # if input.size(1) != 784:\n",
        "        # input_b=binarized(input)\n",
        "        weight_b=binarized(self.weight)\n",
        "        # print(weight_b)\n",
        "        out = nn.functional.linear(input,weight_b)\n",
        "        out_normal = nn.functional.linear(input,self.weight)\n",
        "        if not self.bias is None:\n",
        "            # print(self.bias)\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "            # print(out)\n",
        "        return out\n",
        "\n",
        "list_of_acc_per_run = []\n",
        "for _ in tqdm(range(5)):\n",
        "  # Define the model (1 fully connected layer)\n",
        "  model = nn.Sequential(\n",
        "      nn.Flatten(),  # Flatten the input (28x28) to a vector (784)\n",
        "      BinarizeLinear(28 * 28, 10),  # Fully connected layer with 10 output neurons (for 10 classes)\n",
        "  )\n",
        "\n",
        "  # To store the weights during forward pass\n",
        "  weights_during_forward = []\n",
        "\n",
        "  gradients_ = []\n",
        "\n",
        "  # Hook function to store weights during forward pass\n",
        "  def store_weights_hook(module, input, output):\n",
        "      weights_during_forward.append(module.weight.clone().detach())\n",
        "\n",
        "  # Attach the hook to the nn.Linear layer (model[1])\n",
        "  model[1].register_forward_hook(store_weights_hook)\n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  def train(model, device, train_loader, optimizer, epoch):\n",
        "      model.train()\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          optimizer.zero_grad()  # Zero the gradients\n",
        "          output = model(data)   # Forward pass\n",
        "          loss = criterion(output, target)  # Calculate loss\n",
        "          loss.backward()  # Backward pass (calculate gradients)\n",
        "          for param in model.parameters():\n",
        "              gradients_.append(param.grad)\n",
        "          optimizer.step()  # Update the model parameters\n",
        "\n",
        "\n",
        "  # Evaluation loop\n",
        "  def test(model, device, test_loader):\n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      correct = 0\n",
        "      with torch.no_grad():\n",
        "          for data, target in test_loader:\n",
        "              data, target = data.to(device), target.to(device)\n",
        "              output = model(data)\n",
        "              test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "              pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "              correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "      return 100. * correct / len(test_loader.dataset)\n",
        "  # Check if CUDA is available and use GPU if possible\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Train and test the model\n",
        "  for epoch in range(8):\n",
        "      train(model, device, train_loader, optimizer, epoch)\n",
        "      acc_list.append(test(model, device, test_loader))\n",
        "  list_of_acc_per_run.append(acc_list)\n"
      ]
    }
  ]
}